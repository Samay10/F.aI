# -*- coding: utf-8 -*-
"""LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O1MbwUFGaY14dlf2H2J8dtg8oypwnT-f
"""

# setting up the environment
!pip install transformers datasets torch
!pip install accelerate

# load pre-trained model and tokenizer
from transformers import AutoModelForCausalLM, AutoTokenizer

# load GPT-2 model and tokenizer from Hugging Face
model_name = "distilgpt2"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

#Manually set the pad token to the eos token
tokenizer.pad_token = tokenizer.eos_token

# prepare the dataset
from datasets import load_dataset

# load wikitext-2 dataset
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")

# show a sample
dataset["train"][0]

# tokenize the dataset - turns raw text into numerical format that the model can understand
def tokenize_function(examples):
  # Tokenize the text and create labels
  tokenized = tokenizer(examples["text"], return_tensors="pt", truncation=True, padding="max_length", max_length=256)
  # Create labels - shifted input_ids
  tokenized["labels"] = tokenized["input_ids"].clone()
  return tokenized

# apply the tokenizer to the dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(["text"])  # remove raw text to save memory

# show a sample of tokenized data
tokenized_datasets["train"][0]

# define the training arguments
# batch size, number of epochs and output directory
from transformers import TrainingArguments, IntervalStrategy

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=4,
    logging_dir="./logs",
    logging_steps=200,
    save_steps=500,
    evaluation_strategy="steps",     # Changed to 'steps'
    eval_steps=500,                  # Evaluate every 500 steps
    load_best_model_at_end=True,
    gradient_accumulation_steps=4,
    fp16=True,
    do_train=True,
    do_eval=True
)

# initalize the trainer
from transformers import Trainer, TrainingArguments

trainer = Trainer(
    model=model,                                      # the model to train
    args=training_args,                               # training arguments
    train_dataset=tokenized_datasets["train"],        # training dataset
    eval_dataset=tokenized_datasets["validation"],    # evaluating the dataset
)

# start training
trainer.train()

# Evaluate the model after training
results = trainer.evaluate()

# Display results
print(results)

# Save the model and tokenizer
model.save_pretrained("./final_model")
tokenizer.save_pretrained("./final_model")

# Generate text from the model
input_text = "Once upon a time"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

# Generate a response
output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)

# Decode the output tokens to text
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

print(generated_text)

from flask import Flask, request
import torch

app = Flask(__name__)

# Load the model
model = AutoModelForCausalLM.from_pretrained("./final_model")
tokenizer = AutoTokenizer.from_pretrained("./final_model")

@app.route("/chat", methods=["POST"])
def chat():
    user_input = request.json.get("input")
    input_ids = tokenizer.encode(user_input, return_tensors="pt")
    output = model.generate(input_ids, max_length=100, num_return_sequences=1)
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    return {"response": generated_text}

if __name__ == "__main__":
    app.run(debug=True)